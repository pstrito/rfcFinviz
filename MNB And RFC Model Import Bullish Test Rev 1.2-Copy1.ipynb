{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB And RFC Model Import Bullish Test Rev 1.2\n",
    "\n",
    "This will test to see if we can import a classification model and apply it to a different data set. The original code was from a PyCaret example.\n",
    "\n",
    "NOTE: There are two aspect that need to be remembered. The first is the data manipulation to get it ready to get processed by the model and the import of the model itself.\n",
    "\n",
    "Also there is the model itself and the vocabulary that accompanies the model. When the model is created it also creates a vocabulary from the training data. If the model and the vocabulary are the same name then the model and vocabulary or saved under the same name and one file. If they have different names then both files must be save and used together in the same was as when the model was created.\n",
    "\n",
    "https://towardsdatascience.com/nlp-classification-in-python-pycaret-approach-vs-the-traditional-approach-602d38d29f06\n",
    "\n",
    "https://github.com/prateek025/SMS_Spam_Ham/blob/master/SMS_Spam_Ham_Raw.csv\n",
    "\n",
    "https://github.com/prateek025/SMS_Spam_Ham/blob/master/Spam-Ham.ipynb\n",
    "\n",
    "https://pycaret.org/tune-model/\n",
    "\n",
    "For first time uses you most likely will need to install several different libraries as well as update/upgrade some of the libraries you already have. \n",
    "\n",
    "With the installation of pycaret, if you upgraded numpy, it removes the latest version of numpy and references the previous version. To get around this upgrade numpy after the pycaret library install.\n",
    "\n",
    "You might have to intall the pandas-profiling library. To do so run the following:\n",
    "pip install pandas-profiling --user\n",
    "\n",
    "To find the library version:\n",
    "print(scipy.__version__)\n",
    "\n",
    "To upgrade to the latest version:\n",
    "pip install scipy --upgrade --user\n",
    "\n",
    "To install pycaret:\n",
    "pip install pycaret --user\n",
    "\n",
    "To install spacy (for pycaret):\n",
    "pip install --user https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
    "\n",
    "Correct the errors listed as a result of loading the pycaret library. Most of them will be fixed by either loading the latest version or by installing the missing library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revision History:\n",
    "\n",
    "Model Import Test 0.0 - 1.1\n",
    "\n",
    "Imports the NBC model and runs the model on a data set with having to create a new vocabulary list and model. \n",
    "0.1 - Throws the following error when a different data set is applied to a previously saved model:\n",
    "   \"ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 202 is different from 213)\"\n",
    "   - throws an error if the data set to be analyzed has more columns that the data set used to create the model. This \"could\" mean that the original model did not have as many words. Test this theory we will create the model with the larger vocbulary and use it on the data set with the smaller vocabulary.\n",
    "   - Adds method to pick which model and which data set to be evaluated. (Done)\n",
    "   - Adds Gradient model (Not done)\n",
    "   - Adds accuracy predictions (Not Done)\n",
    "0.2 - runs the model on a single record - didn't work\n",
    "0.3 - adjusted the columns by adjusting the stopwords - worked\n",
    "0.4 - tried inputing a string into the .predict function - didn't work\n",
    "0.5 - stripped out all of the other models and left NB Mulitnomial classifier and used vector.vocabulary & changed vectorizer also vectorized only the body column instead of all of them.  - worked **** THIS STEP WAS KEY ****\n",
    "0.6 - added changes in 0.5 to a full version with all models - worked\n",
    "0.7 - will try to remove all columns except the body and sentiment (independent and dependent variables) essentially did this by vectorizing only the body column in the model creation and in the vectorizing of the new data set.\n",
    "0.8 - outputs model to a .sav file.\n",
    "0.9 - The original example only had one independent variable and one dependent variable. The data set that I was importing had multiple and was making all of them dependent variables. This was causing the dimensional problems. Need to export the model with only one independent variable and one dependent variable, read it in where the model is not generated and is only read in and then apply it to a new data set and see if it runs. - worked/successful\n",
    "1.0 - preprocesses the new data in the same way as the data was that created the model; joins the predicted outcomes with the original file to perform accuracy checks (completed)\n",
    "1.1 - Attemps to inport optimized MultinomialNB model; attempts to import optimized RandomForestClassifier model\n",
    "\n",
    "MNB And RFC Model Import Bullish Test Rev 1.2\n",
    "1.2 - add the RandomForestClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_profiling\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHODS\n",
    "\n",
    "def getData(name):\n",
    "    df1 = pd.DataFrame() # defines df1 as a dataframe\n",
    "    df1 = pd.read_csv(name, header = 0)\n",
    "    return df1\n",
    "\n",
    "# displays a list of file with on a defined suffix/extension       \n",
    "def list_dir_files(relevant_path, exten):\n",
    "    # https://clay-atlas.com/us/blog/2019/10/27/python-english-tutorial-solved-unicodeescape-error-escape-syntaxerror/?doing_wp_cron=1618286551.1528689861297607421875\n",
    "    #need to change \\ to /\n",
    "\n",
    "    # uses os.listdir to display only .csv files\n",
    "    import os\n",
    "    \n",
    "    included_extensions = [exten]\n",
    "    file_names = [fn for fn in os.listdir(relevant_path)\n",
    "              if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "    print('Path: ', relevant_path)\n",
    "\n",
    "    for f in file_names:\n",
    "        print(f)\n",
    "        \n",
    "# DATA PREPARATION 1 - cleans up the dataframe: drops duplicate records; removes duplicate headers; removes unnecessary columns; renames body column;\n",
    "#separates the sentiment column into Bullish, None, Bearish; copies df1 to df2\n",
    "\n",
    "def DataPrep(df1):\n",
    "    \n",
    "    df1 = df1.drop_duplicates() # removes duplicate records\n",
    "    len(df1)\n",
    "\n",
    "    column = 'symbol'\n",
    "    df1.drop(df1[df1['symbol'] == column].index, inplace=True) #removes duplicate headers\n",
    "\n",
    "    df1 = df1.reset_index(drop = True) # resets the index\n",
    "\n",
    "    #Note: symbol and created_at will be needed for app\n",
    "    df1.drop(['symbol', 'created_at', 'followers'], inplace = True, axis = 1) #deletes columns that are not needed for creating the model\n",
    "    print(len(df1))\n",
    "\n",
    "    df1.rename(columns = {'body' : 'body_original'}, inplace = True) #renmaes body as body_original\n",
    "    display(df1.head())\n",
    "    df1 = pd.get_dummies(df1, columns = ['sentiment'], drop_first = False) # drop_first is false to get all three possibilities (column sentiments)\n",
    "\n",
    "    df2 = df1\n",
    "\n",
    "    df2 = df2[['sentiment_Bullish', 'sentiment_None' , 'sentiment_Bearish', 'body_original']] #reorders the columns\n",
    "\n",
    "    print(df2.columns)\n",
    "\n",
    "    print('Data Prep 1 completed. (df clean up; drop duplicates records, headers, certain rows; reorders columns) \\n')\n",
    "\n",
    "    # DATA PREPARATION 2 - cleaning up the comments/tweets: Remove HTTP tags, Converts all to lower case, Removes punctuation;\n",
    "    #Removes unicodes\n",
    "\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # Remove HTTP tags\n",
    "    %time df2['body_Processed'] = df2['body_original'].map(lambda x : ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))\n",
    "    print('HTTP tags removed. \\n')\n",
    "\n",
    "    #Lower Case\n",
    "    %time df2['body_Processed'] = df2['body_Processed'].map(lambda x: x.lower())\n",
    "    print('Converted to lower case. \\n')\n",
    "\n",
    "    #Remove punctuations\n",
    "    %time df2['body_Processed'] = df2['body_Processed'].map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    print('Removed punctuatuations. \\n')\n",
    "\n",
    "    #Remove unicodes\n",
    "    %time df2['body_Processed'] = df2['body_Processed'].map(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "\n",
    "    df2.head()\n",
    "    print('Removed unicodes. \\n')\n",
    "\n",
    "    print('Data Prep 2 completed. (Removal of HTTP tags, punctuation, unicodes; lower case conversion; \\n')\n",
    "\n",
    "    # DATA PREPARATION 3 - lemmatization/stopwords: Removes stopwords, Lemmatizes the words, Removes stopwords from lemmatizaation.\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    #adds new stopwords to list\n",
    "    new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270',\n",
    "                  '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "                 '1600', '993', '392', '98', '00', '1601', 'amd', 'aapl', '03', '10', '100',\n",
    "                  '15', '18', '19', '20', '2021', '57', '0', '5', '11', 'qcom', 'hon', 'ibm',\n",
    "                 'intel', '05', '12', '13', '14', '17', '21', '22', '30', '50', 'intel']\n",
    "\n",
    "    '''new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', '00 call', '00 call entry',]\n",
    "    '''\n",
    "\n",
    "    '''new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270',\n",
    "                  '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "                 '1600', '993', '392', '98', '00', 'amd', 'aapl']'''\n",
    "\n",
    "    for w in new_stop_words:\n",
    "        stop_words.append(w)\n",
    "\n",
    "    #print(stop_words)\n",
    "\n",
    "    #removes the stopwords from the column body_Processed\n",
    "    %time df2['body_Processed'] = df2['body_Processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "    df2.head()\n",
    "\n",
    "    df = df2\n",
    "    # Lemmatize the text\n",
    "    lemmer = WordNetLemmatizer()\n",
    "\n",
    "    import nltk #not in original code\n",
    "    nltk.download('wordnet') #not in original code\n",
    "\n",
    "    %time df2['body_Processed'] = df2['body_Processed'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "    df2.head()\n",
    "\n",
    "    #Removing Stop words again after Lemmatize\n",
    "    %time df2['body_Processed'] = df2['body_Processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "    display(df2.head())\n",
    "    print('Data Prep #3 completed (removal of stopwords and lemmatization) ...\\n')\n",
    "    \n",
    "    return df2\n",
    "\n",
    "#function to prepare Confusion Matrix, RoC-AUC curve, and relvant statistics\n",
    "def clf_report(Y_test, Y_pred, probs):\n",
    "    print(\"\\n\", \"Confusion Matrix\")\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    #print(\"\\n\", cm, \"\\n\")\n",
    "    sns.heatmap(cm, square=True, annot=True, cbar=False, fmt = 'g', cmap='RdBu',\n",
    "                #xticklabels=['ham', 'spam'], yticklabels=['ham', 'spam'])\n",
    "                #xticklabels=['Bullish', 'Non-bullish'], yticklabels=['Bullish', 'Non-bullish'])\n",
    "                xticklabels=['None', 'Non-None'], yticklabels=['None', 'Non-None'])\n",
    "\n",
    "    plt.xlabel('true label')\n",
    "    plt.ylabel('predicted label')\n",
    "    plt.show()\n",
    "    print(\"\\n\", \"Classification Report\", \"\\n\")\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    print(\"Overall Accuracy : \", round(accuracy_score(Y_test, Y_pred) * 100, 2))\n",
    "    print(\"Precision Score : \", round(precision_score(Y_test, Y_pred, average='binary') * 100, 2))\n",
    "    print(\"Recall Score : \", round(recall_score(Y_test, Y_pred, average='binary') * 100, 2))\n",
    "    preds = probs[:,1] # this is the probability for 1, column 0 has probability for 0. Prob(0) + Prob(1) = 1\n",
    "    fpr, tpr, threshold = roc_curve(Y_test, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"AUC : \", round(roc_auc * 100, 2), \"\\n\")\n",
    "    #display(probs)\n",
    "    #print(\"Cutoff Probability : \", preds)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='Best Model on Test Data (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0.0, 1.0], [0, 1],'r--')\n",
    "    plt.xlim([-0.1, 1.1])\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('RoC-AUC on Test Data')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Log_ROC')\n",
    "    plt.show()\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "\n",
    "def accuracy(dfPred):\n",
    "    i = 0\n",
    "    right_score = 0\n",
    "    wrong_score = 0\n",
    "    while i < len(dfPred):\n",
    "        if dfPred.iloc[i,5] == dfPred.iloc[i,0]:\n",
    "            right_score += 1\n",
    "        else:\n",
    "            wrong_score += 1\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    print('Total Correct: ', right_score, '; Percent Correct: ', int(right_score/len(dfPred) * 1000)/10, '%')\n",
    "    print('Total Incorrect: ', wrong_score, '; Percent Incorrrect: ', int(wrong_score/len(dfPred) * 1000)/10, '%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Naive Bayes Classifier\n",
      "Path:  C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Sentiment/rfcFinviz\n",
      "5-22 and 6-01 tech search stocktwits-Copy1 MNB Model-Copy1.sav\n",
      "5-22 and 6-01 tech search stocktwits-Copy1 MNB Vocab-Copy1.sav\n",
      "rfc_optimized_5-22 and 6-01 tech search stocktwits-Copy1 Model-Copy1.sav\n",
      "rfc_optimized_5-22 and 6-01 tech search stocktwits-Copy1 Vocab-Copy1.sav\n",
      "\n",
      "What MNB model file do you want to load? \n",
      "5-22 and 6-01 tech search stocktwits-Copy1 MNB Model-Copy1.sav\n",
      "\n",
      "What MNB vocabulary file do you want to load? \n",
      "5-22 and 6-01 tech search stocktwits-Copy1 MNB Vocab-Copy1.sav\n",
      "\n",
      "What RFC model file do you want to load? \n",
      "rfc_optimized_5-22 and 6-01 tech search stocktwits-Copy1 Model-Copy1.sav\n",
      "\n",
      "What RFC vocabulary file do you want to load? \n",
      "rfc_optimized_5-22 and 6-01 tech search stocktwits-Copy1 Vocab-Copy1.sav\n",
      "Path:  C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/Model Import Tests\n",
      "2021-05-22 tech search stocktwits-Copy1.csv\n",
      "2021-05-22 tech search stocktwits.csv\n",
      "2021-06-01 tech search stocktwits-Copy1.csv\n",
      "5-22 and 6-01 tech search stocktwits.csv\n",
      "RFC_bullish_dfPred.csv\n",
      "tech stockTwit 03112021-Copy1.csv\n",
      "tech stockTwit 03112021.csv\n",
      "What file do you want to perform sentiment analysis on? \n",
      "5-22 and 6-01 tech search stocktwits.csv\n",
      "4620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messageID</th>\n",
       "      <th>body_original</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337302427</td>\n",
       "      <td>$AMD $ATNF $HITID $INTC $AMC ..🚀🚀Let&amp;#39;s go ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>05:53:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>337301864</td>\n",
       "      <td>$AMD $ATNF Our team is up 1253% yesterday so f...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>05:48:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>337301818</td>\n",
       "      <td>$amd $atnf $intc ..Started trading 5 months ag...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>05:47:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>337300512</td>\n",
       "      <td>$amd $atnf $hitid $intc $amc  ..Started tradin...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>05:35:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>337300175</td>\n",
       "      <td>$SPY $ATNF $AMC $INTC..Mark this post... I’m g...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>05:31:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   messageID                                       body_original sentiment  \\\n",
       "0   337302427  $AMD $ATNF $HITID $INTC $AMC ..🚀🚀Let&#39;s go ...      None   \n",
       "1   337301864  $AMD $ATNF Our team is up 1253% yesterday so f...   Bullish   \n",
       "2   337301818  $amd $atnf $intc ..Started trading 5 months ag...   Bullish   \n",
       "3   337300512  $amd $atnf $hitid $intc $amc  ..Started tradin...      None   \n",
       "4   337300175  $SPY $ATNF $AMC $INTC..Mark this post... I’m g...   Bullish   \n",
       "\n",
       "         date      time  \n",
       "0  2021-06-01  05:53:17  \n",
       "1  2021-06-01  05:48:16  \n",
       "2  2021-06-01  05:47:49  \n",
       "3  2021-06-01  05:35:24  \n",
       "4  2021-06-01  05:31:49  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment_Bullish', 'sentiment_None', 'sentiment_Bearish',\n",
      "       'body_original'],\n",
      "      dtype='object')\n",
      "Data Prep 1 completed. (df clean up; drop duplicates records, headers, certain rows; reorders columns) \n",
      "\n",
      "Wall time: 69.8 ms\n",
      "HTTP tags removed. \n",
      "\n",
      "Wall time: 2 ms\n",
      "Converted to lower case. \n",
      "\n",
      "Wall time: 11 ms\n",
      "Removed punctuatuations. \n",
      "\n",
      "Wall time: 12 ms\n",
      "Removed unicodes. \n",
      "\n",
      "Data Prep 2 completed. (Removal of HTTP tags, punctuation, unicodes; lower case conversion; \n",
      "\n",
      "Wall time: 258 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 428 ms\n",
      "Wall time: 197 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_Bullish</th>\n",
       "      <th>sentiment_None</th>\n",
       "      <th>sentiment_Bearish</th>\n",
       "      <th>body_original</th>\n",
       "      <th>body_Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$AMD $ATNF $HITID $INTC $AMC ..🚀🚀Let&amp;#39;s go ...</td>\n",
       "      <td>atnf hitid amc let go big chat tradethemomentu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$AMD $ATNF Our team is up 1253% yesterday so f...</td>\n",
       "      <td>atnf team 1253 yesterday far small cap play gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$amd $atnf $intc ..Started trading 5 months ag...</td>\n",
       "      <td>atnf started trading month ago 4k made profit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$amd $atnf $hitid $intc $amc  ..Started tradin...</td>\n",
       "      <td>atnf hitid amc started trading 4 month ago 3k ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$SPY $ATNF $AMC $INTC..Mark this post... I’m g...</td>\n",
       "      <td>spy atnf amc mark post going follower end summ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_Bullish  sentiment_None  sentiment_Bearish  \\\n",
       "0                  0               1                  0   \n",
       "1                  1               0                  0   \n",
       "2                  1               0                  0   \n",
       "3                  0               1                  0   \n",
       "4                  1               0                  0   \n",
       "\n",
       "                                       body_original  \\\n",
       "0  $AMD $ATNF $HITID $INTC $AMC ..🚀🚀Let&#39;s go ...   \n",
       "1  $AMD $ATNF Our team is up 1253% yesterday so f...   \n",
       "2  $amd $atnf $intc ..Started trading 5 months ag...   \n",
       "3  $amd $atnf $hitid $intc $amc  ..Started tradin...   \n",
       "4  $SPY $ATNF $AMC $INTC..Mark this post... I’m g...   \n",
       "\n",
       "                                      body_Processed  \n",
       "0  atnf hitid amc let go big chat tradethemomentu...  \n",
       "1  atnf team 1253 yesterday far small cap play gr...  \n",
       "2  atnf started trading month ago 4k made profit ...  \n",
       "3  atnf hitid amc started trading 4 month ago 3k ...  \n",
       "4  spy atnf amc mark post going follower end summ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prep #3 completed (removal of stopwords and lemmatization) ...\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialNB' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fd9f2562c72b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;31m###############################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body_Processed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#converts the strings into vectors for sentiment analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;31m###############################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultinomialNB' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, precision_score, recall_score,  accuracy_score, precision_recall_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "###################\n",
    "#HERE IS WHERE THE SAVED MODEL AND VOCAB FILES ARE LOADED WITH PICKLE\n",
    "###################\n",
    "\n",
    "print(\"\\n\", 'Naive Bayes Classifier')\n",
    "\n",
    "#relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/Model Import Tests'\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Sentiment/rfcFinviz'\n",
    "exten = 'sav'\n",
    "list_dir_files(relevant_path, exten)\n",
    "\n",
    "MNB_ModelFileToLoad = input('\\nWhat MNB model file do you want to load? \\n')\n",
    "MNB_VocabFileToLoad = input('\\nWhat MNB vocabulary file do you want to load? \\n')\n",
    "\n",
    "RFC_ModelFileToLoad = input('\\nWhat RFC model file do you want to load? \\n')\n",
    "RFC_VocabFileToLoad = input('\\nWhat RFC vocabulary file do you want to load? \\n')\n",
    "\n",
    "'''\n",
    "#more recent but smaller data set\n",
    "ModelFileToLoad = '2021-05-22 tech search stocktwitsModel.sav'\n",
    "VocabFileToLoad = '2021-05-22 tech search stocktwitsVocab.sav'\n",
    "\n",
    "ModelFileToLoad = 'tech stockTwit 03112021Model.sav'\n",
    "VocabFileToLoad = 'tech stockTwit 03112021Vocab.sav'\n",
    "\n",
    "ModelFileToLoad = 'mnb_optimized_5-22 and 6-01 tech search stocktwitsModel.sav'\n",
    "VocabFileToLoad = 'mnb_optimized_5-22 and 6-01 tech search stocktwitsVocab.sav'\n",
    "'''##\n",
    "\n",
    "MNB_loaded_model = pickle.load(open(MNB_ModelFileToLoad, 'rb')) #The saved MNB model is loaded\n",
    "MNB_loaded_vocab = pickle.load(open(MNB_VocabFileToLoad, 'rb')) #The saved MNB vocab is loaded\n",
    "\n",
    "RFC_loaded_model = pickle.load(open(RFC_ModelFileToLoad, 'rb')) #The saved RFC model is loaded\n",
    "RFC_loaded_vocab = pickle.load(open(RFC_VocabFileToLoad, 'rb')) #The saved RFC vocab is loaded\n",
    "\n",
    "########\n",
    "# LOADS NEW DATA SET; PROCESSES THE NEW DATA; RUNS THE MODEL ON THE NEW DATA.\n",
    "########\n",
    "\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/Model Import Tests'\n",
    "exten = 'csv'\n",
    "list_dir_files(relevant_path, exten)\n",
    "\n",
    "TestFileToLoad = input('What file do you want to perform sentiment analysis on? \\n')\n",
    "\n",
    "#TestFileToLoad = '2021-05-22 tech search stocktwits.csv'\n",
    "df1 = getData(relevant_path + '/' + TestFileToLoad) #returns df; reads csv file into df\n",
    "\n",
    "#tf_vectorizer = CountVectorizer(vocabulary=tf_vectorizer.vocabulary_)\n",
    "#tf_vectorizer = MNB_loaded_vocab\n",
    "tf_vectorizer = RFC_loaded_vocab\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "#Data Preparation\n",
    "##############################\n",
    "df1 = df1.drop_duplicates() # removes duplicate records\n",
    "len(df1)\n",
    "\n",
    "column = 'symbol'\n",
    "df1.drop(df1[df1['symbol'] == column].index, inplace=True) #removes duplicate headers\n",
    "\n",
    "df1 = df1.reset_index(drop = True) # resets the index\n",
    "\n",
    "#Note: symbol and created_at will be needed for app\n",
    "df1.drop(['symbol', 'created_at', 'followers'], inplace = True, axis = 1) #deletes columns that are not needed for creating the model\n",
    "print(len(df1))\n",
    "\n",
    "df1.rename(columns = {'body' : 'body_original'}, inplace = True) #renmaes body as body_original\n",
    "display(df1.head())\n",
    "df1 = pd.get_dummies(df1, columns = ['sentiment'], drop_first = False) # drop_first is false to get all three possibilities (column sentiments)\n",
    "\n",
    "df2 = df1\n",
    "\n",
    "df2 = df2[['sentiment_Bullish', 'sentiment_None' , 'sentiment_Bearish', 'body_original']] #reorders the columns\n",
    "\n",
    "print(df2.columns)\n",
    "\n",
    "print('Data Prep 1 completed. (df clean up; drop duplicates records, headers, certain rows; reorders columns) \\n')\n",
    "\n",
    "# DATA PREPARATION 2 - cleaning up the comments/tweets: Remove HTTP tags, Converts all to lower case, Removes punctuation;\n",
    "#Removes unicodes\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Remove HTTP tags\n",
    "%time df2['body_Processed'] = df2['body_original'].map(lambda x : ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))\n",
    "print('HTTP tags removed. \\n')\n",
    "\n",
    "#Lower Case\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x: x.lower())\n",
    "print('Converted to lower case. \\n')\n",
    "\n",
    "#Remove punctuations\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "print('Removed punctuatuations. \\n')\n",
    "\n",
    "#Remove unicodes\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "\n",
    "df2.head()\n",
    "print('Removed unicodes. \\n')\n",
    "\n",
    "print('Data Prep 2 completed. (Removal of HTTP tags, punctuation, unicodes; lower case conversion; \\n')\n",
    "\n",
    "# DATA PREPARATION 3 - lemmatization/stopwords: Removes stopwords, Lemmatizes the words, Removes stopwords from lemmatizaation.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#adds new stopwords to list\n",
    "new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270',\n",
    "     '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "    '1600', '993', '392', '98', '00', '1601', 'amd', 'aapl', '03', '10', '100',\n",
    "     '15', '18', '19', '20', '2021', '57', '0', '5', '11', 'qcom', 'hon', 'ibm',\n",
    "    'intel', '05', '12', '13', '14', '17', '21', '22', '30', '50', 'intel']\n",
    "\n",
    "'''new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', '00 call', '00 call entry',]\n",
    "'''\n",
    "\n",
    "'''new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270',\n",
    "      '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "     '1600', '993', '392', '98', '00', 'amd', 'aapl']'''\n",
    "\n",
    "for w in new_stop_words:\n",
    "    stop_words.append(w)\n",
    "\n",
    "#print(stop_words)\n",
    "\n",
    "#removes the stopwords from the column body_Processed\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "df2.head()\n",
    "\n",
    "# df = df2\n",
    "# Lemmatize the text\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "import nltk #not in original code\n",
    "nltk.download('wordnet') #not in original code\n",
    "\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "df2.head()\n",
    "\n",
    "#Removing Stop words again after Lemmatize\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "display(df2.head())\n",
    "print('Data Prep #3 completed (removal of stopwords and lemmatization) ...\\n')\n",
    " \n",
    "###############################\n",
    "# APPLIES VOCABULARY AND TOKENIZES THE PREPROCESSED COMMENTS; tf_vectorizer carries the vocabulary and the function??\n",
    "###############################\n",
    "\n",
    "X_new = tf_vectorizer.transform(df2['body_Processed']) #converts the strings into vectors for sentiment analysis\n",
    "\n",
    "###############################\n",
    "# MNB MODEL\n",
    "###############################\n",
    "MNB_Y_pred = MNB_loaded_model.predict(X_new) #Runs the MNB model on the new data set\n",
    "\n",
    "display('New Y_pred: ', MNB_Y_pred)\n",
    "\n",
    "df3 = pd.DataFrame(MNB_Y_pred, columns = ['Predicted Bullish Sentiment']) #creates a new df3 from the Y_pred array\n",
    "MNB_dfPred = df2.join(df3) #joins the df2 and df3 dataframes together\n",
    "\n",
    "print('The accuracy for the MNB model is: ')\n",
    "accuracy(MNB_dfPred)\n",
    "\n",
    "###############################\n",
    "# RFC MODEL\n",
    "###############################\n",
    "RFC_Y_pred = RFC_loaded_model.predict(X_new) #Runs RFC model on the new data set\n",
    "df4 = pd.DataFrame(RFC_Y_pred, columns = ['Predicted Bullish Sentiment']) #creates a new df4 from the Y-Pred array\n",
    "RFC_dfPred = df2.join(df4) #joins the df2 and df4 dataframes togther\n",
    "\n",
    "print('The accuracy for the RFC model is: ')\n",
    "accuracy(RFC_dfPred)\n",
    "\n",
    "#writes the RFC_dfPred to a csv file\n",
    "RFC_dfPred.to_csv('RFC_bullish_dfPred.csv', index = False, encoding = 'utf-8')\n",
    "print('The csv file was written. File name: ', 'RFC_bullish_dfPred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_Bullish</th>\n",
       "      <th>sentiment_None</th>\n",
       "      <th>sentiment_Bearish</th>\n",
       "      <th>body_original</th>\n",
       "      <th>body_Processed</th>\n",
       "      <th>Predicted Bullish Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$AMD $ATNF $HITID $INTC $AMC ..🚀🚀Let&amp;#39;s go ...</td>\n",
       "      <td>atnf hitid amc let go big chat tradethemomentu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$AMD $ATNF Our team is up 1253% yesterday so f...</td>\n",
       "      <td>atnf team 1253 yesterday far small cap play gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$amd $atnf $intc ..Started trading 5 months ag...</td>\n",
       "      <td>atnf started trading month ago 4k made profit ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$amd $atnf $hitid $intc $amc  ..Started tradin...</td>\n",
       "      <td>atnf hitid amc started trading 4 month ago 3k ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$SPY $ATNF $AMC $INTC..Mark this post... I’m g...</td>\n",
       "      <td>spy atnf amc mark post going follower end summ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4615</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$MU WDC has outperformed MU by 10% over 2 days...</td>\n",
       "      <td>wdc outperformed 2 day two day insane big run ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4616</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$MU GREEEN BABY LET&amp;#39;S GO!</td>\n",
       "      <td>greeen baby let go</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4617</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$MU we’re outperforming the SMH? Never thought...</td>\n",
       "      <td>outperforming smh never thought see day</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4618</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@DocOctagon $MU I&amp;#39;m hoping that with Zinsn...</td>\n",
       "      <td>hoping zinsner cfo barclays wednesday morning ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4619</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>It&amp;#39;s Time To Buy Weed Stocks. $CRK $MU htt...</td>\n",
       "      <td>time buy weed stock crk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4620 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment_Bullish  sentiment_None  sentiment_Bearish  \\\n",
       "0                     0               1                  0   \n",
       "1                     1               0                  0   \n",
       "2                     1               0                  0   \n",
       "3                     0               1                  0   \n",
       "4                     1               0                  0   \n",
       "...                 ...             ...                ...   \n",
       "4615                  0               1                  0   \n",
       "4616                  1               0                  0   \n",
       "4617                  0               1                  0   \n",
       "4618                  1               0                  0   \n",
       "4619                  0               1                  0   \n",
       "\n",
       "                                          body_original  \\\n",
       "0     $AMD $ATNF $HITID $INTC $AMC ..🚀🚀Let&#39;s go ...   \n",
       "1     $AMD $ATNF Our team is up 1253% yesterday so f...   \n",
       "2     $amd $atnf $intc ..Started trading 5 months ag...   \n",
       "3     $amd $atnf $hitid $intc $amc  ..Started tradin...   \n",
       "4     $SPY $ATNF $AMC $INTC..Mark this post... I’m g...   \n",
       "...                                                 ...   \n",
       "4615  $MU WDC has outperformed MU by 10% over 2 days...   \n",
       "4616                      $MU GREEEN BABY LET&#39;S GO!   \n",
       "4617  $MU we’re outperforming the SMH? Never thought...   \n",
       "4618  @DocOctagon $MU I&#39;m hoping that with Zinsn...   \n",
       "4619  It&#39;s Time To Buy Weed Stocks. $CRK $MU htt...   \n",
       "\n",
       "                                         body_Processed  \\\n",
       "0     atnf hitid amc let go big chat tradethemomentu...   \n",
       "1     atnf team 1253 yesterday far small cap play gr...   \n",
       "2     atnf started trading month ago 4k made profit ...   \n",
       "3     atnf hitid amc started trading 4 month ago 3k ...   \n",
       "4     spy atnf amc mark post going follower end summ...   \n",
       "...                                                 ...   \n",
       "4615  wdc outperformed 2 day two day insane big run ...   \n",
       "4616                                 greeen baby let go   \n",
       "4617            outperforming smh never thought see day   \n",
       "4618  hoping zinsner cfo barclays wednesday morning ...   \n",
       "4619                            time buy weed stock crk   \n",
       "\n",
       "      Predicted Bullish Sentiment  \n",
       "0                               0  \n",
       "1                               0  \n",
       "2                               0  \n",
       "3                               0  \n",
       "4                               0  \n",
       "...                           ...  \n",
       "4615                            0  \n",
       "4616                            0  \n",
       "4617                            0  \n",
       "4618                            0  \n",
       "4619                            0  \n",
       "\n",
       "[4620 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(RFC_dfPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Naive Bayes Classifier\n",
      "1468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messageID</th>\n",
       "      <th>body_original</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333823830</td>\n",
       "      <td>$INTC  Simulated 57.0 dollar  CALLS for Monday...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>16:18:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>333815936</td>\n",
       "      <td>$AMD $MU $INTC $QCOM $NVDA ..Nvda splitting......</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>15:32:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>333801174</td>\n",
       "      <td>5 of 11 $HON $IBM $INTC Dark green arrows indi...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>13:54:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333794667</td>\n",
       "      <td>$INTC  Timing the market with short dated opti...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>13:06:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333790756</td>\n",
       "      <td>Your daily News digest for Intel $INTC https:/...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>12:33:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   messageID                                       body_original sentiment  \\\n",
       "0   333823830  $INTC  Simulated 57.0 dollar  CALLS for Monday...   Bullish   \n",
       "1   333815936  $AMD $MU $INTC $QCOM $NVDA ..Nvda splitting......      None   \n",
       "2   333801174  5 of 11 $HON $IBM $INTC Dark green arrows indi...      None   \n",
       "3   333794667  $INTC  Timing the market with short dated opti...   Bullish   \n",
       "4   333790756  Your daily News digest for Intel $INTC https:/...      None   \n",
       "\n",
       "         date      time  \n",
       "0  2021-05-22  16:18:23  \n",
       "1  2021-05-22  15:32:39  \n",
       "2  2021-05-22  13:54:06  \n",
       "3  2021-05-22  13:06:41  \n",
       "4  2021-05-22  12:33:05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment_Bullish', 'sentiment_None', 'sentiment_Bearish',\n",
      "       'body_original'],\n",
      "      dtype='object')\n",
      "Data Prep 1 completed. (df clean up; drop duplicates records, headers, certain rows; reorders columns) \n",
      "\n",
      "Wall time: 25 ms\n",
      "HTTP tags removed. \n",
      "\n",
      "Wall time: 1.01 ms\n",
      "Converted to lower case. \n",
      "\n",
      "Wall time: 3.99 ms\n",
      "Removed punctuatuations. \n",
      "\n",
      "Wall time: 5.98 ms\n",
      "Removed unicodes. \n",
      "\n",
      "Data Prep 2 completed. (Removal of HTTP tags, punctuation, unicodes; lower case conversion; \n",
      "\n",
      "Wall time: 87.8 ms\n",
      "Wall time: 148 ms\n",
      "Wall time: 65.2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_Bullish</th>\n",
       "      <th>sentiment_None</th>\n",
       "      <th>sentiment_Bearish</th>\n",
       "      <th>body_original</th>\n",
       "      <th>body_Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$INTC  Simulated 57.0 dollar  CALLS for Monday...</td>\n",
       "      <td>simulated dollar call monday open stockorbit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$AMD $MU $INTC $QCOM $NVDA ..Nvda splitting......</td>\n",
       "      <td>splitting much know debt ceiling impact overal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5 of 11 $HON $IBM $INTC Dark green arrows indi...</td>\n",
       "      <td>dark green arrow indicate strong buy signal da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$INTC  Timing the market with short dated opti...</td>\n",
       "      <td>timing market short dated option burn account ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Your daily News digest for Intel $INTC https:/...</td>\n",
       "      <td>daily news digest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_Bullish  sentiment_None  sentiment_Bearish  \\\n",
       "0                  1               0                  0   \n",
       "1                  0               1                  0   \n",
       "2                  0               1                  0   \n",
       "3                  1               0                  0   \n",
       "4                  0               1                  0   \n",
       "\n",
       "                                       body_original  \\\n",
       "0  $INTC  Simulated 57.0 dollar  CALLS for Monday...   \n",
       "1  $AMD $MU $INTC $QCOM $NVDA ..Nvda splitting......   \n",
       "2  5 of 11 $HON $IBM $INTC Dark green arrows indi...   \n",
       "3  $INTC  Timing the market with short dated opti...   \n",
       "4  Your daily News digest for Intel $INTC https:/...   \n",
       "\n",
       "                                      body_Processed  \n",
       "0       simulated dollar call monday open stockorbit  \n",
       "1  splitting much know debt ceiling impact overal...  \n",
       "2  dark green arrow indicate strong buy signal da...  \n",
       "3  timing market short dated option burn account ...  \n",
       "4                                  daily news digest  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prep #3 completed (removal of stopwords and lemmatization) ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'New Y_pred: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 1, 0], dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct:  885 ; Percent Correct:  60.2 %\n",
      "Total Incorrect:  583 ; Percent Incorrrect:  39.7 %\n"
     ]
    }
   ],
   "source": [
    "# MAIN NOT OPTIMIZED MODEL\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, precision_score, recall_score,  accuracy_score, precision_recall_curve\n",
    "\n",
    "\n",
    "\n",
    "#HERE IS WHERE THE SAVED MODEL AND VOCAB FILES ARE LOADED WITH PICKLE AND RUN\n",
    "print(\"\\n\", 'Naive Bayes Classifier')\n",
    "\n",
    "'''\n",
    "#more recent but smaller data set\n",
    "ModelFileToLoad = '2021-05-22 tech search stocktwitsModel.sav'\n",
    "VocabFileToLoad = '2021-05-22 tech search stocktwitsVocab.sav'\n",
    "'''\n",
    "ModelFileToLoad = 'tech stockTwit 03112021Model.sav'\n",
    "VocabFileToLoad = 'tech stockTwit 03112021Vocab.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(ModelFileToLoad, 'rb')) #The saved model is loaded\n",
    "loaded_vocab = pickle.load(open(VocabFileToLoad, 'rb')) #The saved vocab is loaded\n",
    "\n",
    "########\n",
    "# LOADS NEW DATA SET; PROCESSES THE NEW DATA; RUNS THE MODEL ON THE NEW DATA.\n",
    "########\n",
    "\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/Model Import Tests'\n",
    "df3 = getData(relevant_path + '/' + '2021-05-22 tech search stocktwits.csv') #returns df; reads csv file into df\n",
    "\n",
    "#tf_vectorizer = CountVectorizer(vocabulary=tf_vectorizer.vocabulary_)\n",
    "tf_vectorizer = loaded_vocab\n",
    "\n",
    "df3 = DataPrep(df3)\n",
    "\n",
    "X_new = tf_vectorizer.transform(df3['body_Processed'])\n",
    "   \n",
    "Y_pred = loaded_model.predict(X_new) #Runs the model on the new data set\n",
    "\n",
    "display('New Y_pred: ', Y_pred)\n",
    "\n",
    "df4 = pd.DataFrame(Y_pred, columns=['Predicted Sentiment']) #creates a new df5 from the Y_pred array\n",
    "dfPred = df3.join(df4) #joins the df3 and df4 dataframes together\n",
    "\n",
    "accuracy(dfPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2417\n"
     ]
    }
   ],
   "source": [
    "print(len(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame(Y_pred, columns=['Predicted Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted Sentiment\n",
       "0                    1\n",
       "1                    1\n",
       "2                    1\n",
       "3                    0\n",
       "4                    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_Bullish</th>\n",
       "      <th>sentiment_None</th>\n",
       "      <th>sentiment_Bearish</th>\n",
       "      <th>body_original</th>\n",
       "      <th>body_Processed</th>\n",
       "      <th>Predicted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$INTC Big Trade - $16 399 800.270 000 shares a...</td>\n",
       "      <td>big trade share</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Large Print $INTC Size: 270000 Price: 60.74 Ti...</td>\n",
       "      <td>large print size price time 1601 amount</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Huge Print $INTC Size: 4033477 Price: 60.74 Ti...</td>\n",
       "      <td>huge print size price time amount</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$AMD common follow ur sibs $INTC $MU</td>\n",
       "      <td>common follow ur sib</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$ITT $INTC $ADBE $OPTT $GLBS  .  .</td>\n",
       "      <td>itt optt glbs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_Bullish  sentiment_None  sentiment_Bearish  \\\n",
       "0                  0               1                  0   \n",
       "1                  0               1                  0   \n",
       "2                  0               1                  0   \n",
       "3                  1               0                  0   \n",
       "4                  1               0                  0   \n",
       "\n",
       "                                       body_original  \\\n",
       "0  $INTC Big Trade - $16 399 800.270 000 shares a...   \n",
       "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...   \n",
       "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...   \n",
       "3               $AMD common follow ur sibs $INTC $MU   \n",
       "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .    \n",
       "\n",
       "                            body_Processed  Predicted Sentiment  \n",
       "0                          big trade share                    1  \n",
       "1  large print size price time 1601 amount                    1  \n",
       "2        huge print size price time amount                    1  \n",
       "3                     common follow ur sib                    0  \n",
       "4                            itt optt glbs                    0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dfPred = df3.join(df4)\n",
    "\n",
    "display(dfPred.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment_Bullish', 'sentiment_None', 'sentiment_Bearish',\n",
      "       'body_original', 'body_Processed', 'Predicted Sentiment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dfPred.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct:  1347 ; Percent Correct:  55.7 %\n",
      "Total Incorrect:  1070 ; Percent Incorrrect:  44.2 %\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "right_score = 0\n",
    "wrong_score = 0\n",
    "while i < len(dfPred):\n",
    "    if dfPred.iloc[i,5] == dfPred.iloc[i,0]:\n",
    "        right_score += 1\n",
    "    else:\n",
    "        wrong_score += 1\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "print('Total Correct: ', right_score, '; Percent Correct: ', int(right_score/len(dfPred) * 1000)/10, '%')\n",
    "print('Total Incorrect: ', wrong_score, '; Percent Incorrrect: ', int(wrong_score/len(dfPred) * 1000)/10, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported the csv file.\n",
      "Index(['symbol', 'messageID ', 'created_at', 'body', 'followers', 'sentiment',\n",
      "       'date', 'time'],\n",
      "      dtype='object')\n",
      "\n",
      " Naive Bayes Classifier\n",
      "2417\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_original</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$INTC Big Trade - $16 399 800.270 000 shares a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large Print $INTC Size: 270000 Price: 60.74 Ti...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Huge Print $INTC Size: 4033477 Price: 60.74 Ti...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$AMD common follow ur sibs $INTC $MU</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$ITT $INTC $ADBE $OPTT $GLBS  .  .</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       body_original sentiment\n",
       "0  $INTC Big Trade - $16 399 800.270 000 shares a...      None\n",
       "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...      None\n",
       "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      None\n",
       "3               $AMD common follow ur sibs $INTC $MU   Bullish\n",
       "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .    Bullish"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment_Bullish', 'sentiment_None', 'sentiment_Bearish',\n",
      "       'body_original'],\n",
      "      dtype='object')\n",
      "Data Prep 1 completed. (df clean up; drop duplicates records, headers, certain rows; reorders columns) \n",
      "\n",
      "Wall time: 35.9 ms\n",
      "HTTP tags removed. \n",
      "\n",
      "Wall time: 995 µs\n",
      "Converted to lower case. \n",
      "\n",
      "Wall time: 6.99 ms\n",
      "Removed punctuatuations. \n",
      "\n",
      "Wall time: 6.01 ms\n",
      "Removed unicodes. \n",
      "\n",
      "Data Prep 2 completed. (Removal of HTTP tags, punctuation, unicodes; lower case conversion; \n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270', '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74', '1600', '993', '392', '98', '00', '1601', 'amd', 'aapl', '03', '10', '100', '15', '18', '19', '20', '2021', '57', '0', '5', '11', 'qcom', 'hon', 'ibm', 'intel', '05', '12', '13', '14', '17', '21', '22', '30', '50']\n",
      "Wall time: 112 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 205 ms\n",
      "Wall time: 85.2 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_Bullish</th>\n",
       "      <th>sentiment_None</th>\n",
       "      <th>sentiment_Bearish</th>\n",
       "      <th>body_original</th>\n",
       "      <th>body_Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$INTC Big Trade - $16 399 800.270 000 shares a...</td>\n",
       "      <td>big trade share</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Large Print $INTC Size: 270000 Price: 60.74 Ti...</td>\n",
       "      <td>large print size price time 1601 amount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Huge Print $INTC Size: 4033477 Price: 60.74 Ti...</td>\n",
       "      <td>huge print size price time amount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$AMD common follow ur sibs $INTC $MU</td>\n",
       "      <td>common follow ur sib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$ITT $INTC $ADBE $OPTT $GLBS  .  .</td>\n",
       "      <td>itt optt glbs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_Bullish  sentiment_None  sentiment_Bearish  \\\n",
       "0                  0               1                  0   \n",
       "1                  0               1                  0   \n",
       "2                  0               1                  0   \n",
       "3                  1               0                  0   \n",
       "4                  1               0                  0   \n",
       "\n",
       "                                       body_original  \\\n",
       "0  $INTC Big Trade - $16 399 800.270 000 shares a...   \n",
       "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...   \n",
       "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...   \n",
       "3               $AMD common follow ur sibs $INTC $MU   \n",
       "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .    \n",
       "\n",
       "                            body_Processed  \n",
       "0                          big trade share  \n",
       "1  large print size price time 1601 amount  \n",
       "2        huge print size price time amount  \n",
       "3                     common follow ur sib  \n",
       "4                            itt optt glbs  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prep #3 completed (removal of stopwords and lemmatization) ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'New Y_pred: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1], dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "#relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files'\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/Model Import Tests'\n",
    "#df1 = pd.read_csv('tech stockTwit 03112021-Copy1.csv') #uses data from stocktwits to train the models\n",
    "#df1 = pd.read_csv('2021-05-22 tech search stocktwits-Copy1.csv') #uses data from stocktwits to train the models\n",
    "\n",
    "filename = '2021-05-22 tech search stocktwits.csv'\n",
    "\n",
    "df1 = getData(relevant_path + '/' + filename) #returns df; reads csv file into df\n",
    "\n",
    "print('Imported the csv file.')\n",
    "\n",
    "print(df1.columns)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, precision_score, recall_score,  accuracy_score, precision_recall_curve\n",
    "\n",
    "#HERE IS WHERE THE SAVED MODEL IS LOADED WITH PICKLE AND RUN\n",
    "print(\"\\n\", 'Naive Bayes Classifier')\n",
    "\n",
    "ModelFileToLoad = '2021-05-22 tech search stocktwitsModel.sav'\n",
    "VocabFileToLoad = '2021-05-22 tech search stocktwitsVocab.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(ModelFileToLoad, 'rb')) #The saved model is loaded\n",
    "loaded_vocab = pickle.load(open(VocabFileToLoad, 'rb')) #The saved vocab is loaded\n",
    "\n",
    "########\n",
    "# LOADS NEW DATA SET; PROCESSES THE NEW DATA; RUNS THE MODEL ON THE NEW DATA.\n",
    "########\n",
    "\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/Model Import Tests'\n",
    "df3 = getData(relevant_path + '/' + 'tech stockTwit 03112021.csv') #returns df; reads csv file into df\n",
    "\n",
    "#tf_vectorizer = CountVectorizer(vocabulary=tf_vectorizer.vocabulary_)\n",
    "tf_vectorizer = loaded_vocab\n",
    "\n",
    "df3 = DataPrep(df3)\n",
    "\n",
    "X_new = tf_vectorizer.transform(df3['body_Processed'])\n",
    "   \n",
    "Y_pred = loaded_model.predict(X_new) #Runs the model on the new data set\n",
    "\n",
    "display('New Y_pred: ', Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported the csv file.\n",
      "Index(['symbol', 'messageID ', 'created_at', 'body', 'followers', 'sentiment',\n",
      "       'date', 'time'],\n",
      "      dtype='object')\n",
      "1468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messageID</th>\n",
       "      <th>body_original</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333823830</td>\n",
       "      <td>$INTC  Simulated 57.0 dollar  CALLS for Monday...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>16:18:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>333815936</td>\n",
       "      <td>$AMD $MU $INTC $QCOM $NVDA ..Nvda splitting......</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>15:32:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>333801174</td>\n",
       "      <td>5 of 11 $HON $IBM $INTC Dark green arrows indi...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>13:54:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333794667</td>\n",
       "      <td>$INTC  Timing the market with short dated opti...</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>13:06:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333790756</td>\n",
       "      <td>Your daily News digest for Intel $INTC https:/...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>12:33:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   messageID                                       body_original sentiment  \\\n",
       "0   333823830  $INTC  Simulated 57.0 dollar  CALLS for Monday...   Bullish   \n",
       "1   333815936  $AMD $MU $INTC $QCOM $NVDA ..Nvda splitting......      None   \n",
       "2   333801174  5 of 11 $HON $IBM $INTC Dark green arrows indi...      None   \n",
       "3   333794667  $INTC  Timing the market with short dated opti...   Bullish   \n",
       "4   333790756  Your daily News digest for Intel $INTC https:/...      None   \n",
       "\n",
       "         date      time  \n",
       "0  2021-05-22  16:18:23  \n",
       "1  2021-05-22  15:32:39  \n",
       "2  2021-05-22  13:54:06  \n",
       "3  2021-05-22  13:06:41  \n",
       "4  2021-05-22  12:33:05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment_Bullish', 'sentiment_None', 'sentiment_Bearish',\n",
      "       'body_original'],\n",
      "      dtype='object')\n",
      "Data Prep 1 completed. \n",
      "\n",
      "Wall time: 24.9 ms\n",
      "HTTP tags removed. \n",
      "\n",
      "Wall time: 1.97 ms\n",
      "Converted to lower case. \n",
      "\n",
      "Wall time: 3.99 ms\n",
      "Removed punctuatuations. \n",
      "\n",
      "Wall time: 5 ms\n",
      "Removed unicodes. \n",
      "\n",
      "Data Prep 2 completed. \n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270', '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74', '1600', '993', '392', '98', '00', 'amd', 'aapl']\n",
      "Wall time: 68 ms\n",
      "Wall time: 133 ms\n",
      "Wall time: 50.1 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_Bullish</th>\n",
       "      <th>sentiment_None</th>\n",
       "      <th>sentiment_Bearish</th>\n",
       "      <th>body_original</th>\n",
       "      <th>body_Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$INTC  Simulated 57.0 dollar  CALLS for Monday...</td>\n",
       "      <td>simulated 57 0 dollar call monday open stockorbit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$AMD $MU $INTC $QCOM $NVDA ..Nvda splitting......</td>\n",
       "      <td>qcom splitting much know debt ceiling impact o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5 of 11 $HON $IBM $INTC Dark green arrows indi...</td>\n",
       "      <td>5 11 hon ibm dark green arrow indicate strong ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$INTC  Timing the market with short dated opti...</td>\n",
       "      <td>timing market short dated option burn account ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Your daily News digest for Intel $INTC https:/...</td>\n",
       "      <td>daily news digest intel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_Bullish  sentiment_None  sentiment_Bearish  \\\n",
       "0                  1               0                  0   \n",
       "1                  0               1                  0   \n",
       "2                  0               1                  0   \n",
       "3                  1               0                  0   \n",
       "4                  0               1                  0   \n",
       "\n",
       "                                       body_original  \\\n",
       "0  $INTC  Simulated 57.0 dollar  CALLS for Monday...   \n",
       "1  $AMD $MU $INTC $QCOM $NVDA ..Nvda splitting......   \n",
       "2  5 of 11 $HON $IBM $INTC Dark green arrows indi...   \n",
       "3  $INTC  Timing the market with short dated opti...   \n",
       "4  Your daily News digest for Intel $INTC https:/...   \n",
       "\n",
       "                                      body_Processed  \n",
       "0  simulated 57 0 dollar call monday open stockorbit  \n",
       "1  qcom splitting much know debt ceiling impact o...  \n",
       "2  5 11 hon ibm dark green arrow indicate strong ...  \n",
       "3  timing market short dated option burn account ...  \n",
       "4                            daily news digest intel  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 103 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bow-TF :'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1468, 211)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>05</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>15 2021</th>\n",
       "      <th>17</th>\n",
       "      <th>...</th>\n",
       "      <th>unusual</th>\n",
       "      <th>unusual option</th>\n",
       "      <th>unusual option activity</th>\n",
       "      <th>volume</th>\n",
       "      <th>week</th>\n",
       "      <th>weekly</th>\n",
       "      <th>weekly call</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   05  10  100  11  12  13  14  15  15 2021  17  ...  unusual  unusual option  \\\n",
       "0   0   0    0   0   0   0   0   0        0   0  ...        0               0   \n",
       "1   0   0    0   0   0   0   0   0        0   0  ...        0               0   \n",
       "2   0   0    0   2   0   0   0   0        0   0  ...        0               0   \n",
       "3   0   0    0   0   0   0   0   0        0   0  ...        0               0   \n",
       "4   0   0    0   0   0   0   0   0        0   0  ...        0               0   \n",
       "\n",
       "   unusual option activity  volume  week  weekly  weekly call  worth  would  \\\n",
       "0                        0       0     0       0            0      0      0   \n",
       "1                        0       0     0       0            0      0      0   \n",
       "2                        0       0     0       0            0      0      0   \n",
       "3                        0       0     0       0            0      0      0   \n",
       "4                        0       0     0       0            0      0      0   \n",
       "\n",
       "   year  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 211 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Naive Bayes Classifier\n",
      "Index(['symbol', 'created_at', 'body', 'followers', 'sentiment'], dtype='object')\n",
      "2417\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_original</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$INTC Big Trade - $16 399 800.270 000 shares a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large Print $INTC Size: 270000 Price: 60.74 Ti...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Huge Print $INTC Size: 4033477 Price: 60.74 Ti...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$AMD common follow ur sibs $INTC $MU</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$ITT $INTC $ADBE $OPTT $GLBS  .  .</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       body_original sentiment\n",
       "0  $INTC Big Trade - $16 399 800.270 000 shares a...      None\n",
       "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...      None\n",
       "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      None\n",
       "3               $AMD common follow ur sibs $INTC $MU   Bullish\n",
       "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .    Bullish"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment_Bullish', 'sentiment_None', 'sentiment_Bearish',\n",
      "       'body_original'],\n",
      "      dtype='object')\n",
      "Data Prep 1 completed. (df clean up; drop duplicates records, headers, certain rows; reorders columns) \n",
      "\n",
      "Wall time: 37.9 ms\n",
      "HTTP tags removed. \n",
      "\n",
      "Wall time: 1.99 ms\n",
      "Converted to lower case. \n",
      "\n",
      "Wall time: 5.01 ms\n",
      "Removed punctuatuations. \n",
      "\n",
      "Wall time: 7.01 ms\n",
      "Removed unicodes. \n",
      "\n",
      "Data Prep 2 completed. (Removal of HTTP tags, punctuation, unicodes; lower case conversion; \n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270', '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74', '1600', '993', '392', '98', '00', '1601', 'amd', 'aapl', '03', '10', '100', '15', '18', '19', '20', '2021', '57', '0', '5', '11', 'qcom', 'hon', 'ibm', 'intel', '05', '12', '13', '14', '17', '21', '22', '30', '50']\n",
      "Wall time: 122 ms\n",
      "Wall time: 194 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 92.8 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_Bullish</th>\n",
       "      <th>sentiment_None</th>\n",
       "      <th>sentiment_Bearish</th>\n",
       "      <th>body_original</th>\n",
       "      <th>body_Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>$INTC Big Trade - $16 399 800.270 000 shares a...</td>\n",
       "      <td>big trade share</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Large Print $INTC Size: 270000 Price: 60.74 Ti...</td>\n",
       "      <td>large print size price time 1601 amount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Huge Print $INTC Size: 4033477 Price: 60.74 Ti...</td>\n",
       "      <td>huge print size price time amount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$AMD common follow ur sibs $INTC $MU</td>\n",
       "      <td>common follow ur sib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$ITT $INTC $ADBE $OPTT $GLBS  .  .</td>\n",
       "      <td>itt optt glbs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_Bullish  sentiment_None  sentiment_Bearish  \\\n",
       "0                  0               1                  0   \n",
       "1                  0               1                  0   \n",
       "2                  0               1                  0   \n",
       "3                  1               0                  0   \n",
       "4                  1               0                  0   \n",
       "\n",
       "                                       body_original  \\\n",
       "0  $INTC Big Trade - $16 399 800.270 000 shares a...   \n",
       "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...   \n",
       "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...   \n",
       "3               $AMD common follow ur sibs $INTC $MU   \n",
       "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .    \n",
       "\n",
       "                            body_Processed  \n",
       "0                          big trade share  \n",
       "1  large print size price time 1601 amount  \n",
       "2        huge print size price time amount  \n",
       "3                     common follow ur sib  \n",
       "4                            itt optt glbs  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prep #3 completed (removal of stopwords and lemmatization) ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'New Y_pred: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1], dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment_Bullish', 'sentiment_None', 'sentiment_Bearish',\n",
      "       'body_original', 'body_Processed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation for Sentiment Analysis\n",
    "\n",
    "#relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files'\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/Model Import Tests'\n",
    "#df1 = pd.read_csv('tech stockTwit 03112021-Copy1.csv') #uses data from stocktwits to train the models\n",
    "#df1 = pd.read_csv('2021-05-22 tech search stocktwits-Copy1.csv') #uses data from stocktwits to train the models\n",
    "\n",
    "'''print('Here is a list of the csv files to choose from: \\n')\n",
    "exten = 'csv'\n",
    "list_dir_files(relevant_path, exten)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "name = input('\\nWhat file do you want to use? ')\n",
    "df1 = getData(relevant_path + '/' + name) #returns df; reads csv file into df'''\n",
    "\n",
    "filename = '2021-05-22 tech search stocktwits.csv'\n",
    "\n",
    "df1 = getData(relevant_path + '/' + filename) #returns df; reads csv file into df\n",
    "\n",
    "print('Imported the csv file.')\n",
    "\n",
    "print(df1.columns)\n",
    "\n",
    "# DATA PREPARATION 1 - cleans up the dataframe: drops duplicate records; removes duplicate headers; removes unnecessary columns; renames body column;\n",
    "#separates the sentiment column into Bullish, None, Bearish; copies df1 to df2\n",
    "\n",
    "df1 = df1.drop_duplicates() # removes duplicate records\n",
    "len(df1)\n",
    "\n",
    "column = 'symbol'\n",
    "df1.drop(df1[df1['symbol'] == column].index, inplace=True) #removes duplicate headers\n",
    "\n",
    "df1 = df1.reset_index(drop = True) # resets the index\n",
    "\n",
    "#Note: symbol and cretaed_at will be needed for app\n",
    "df1.drop(['symbol', 'created_at', 'followers'], inplace = True, axis = 1) #deletes columns that are not needed for creating the model\n",
    "print(len(df1))\n",
    "\n",
    "df1.rename(columns = {'body' : 'body_original'}, inplace = True) #renmaes body as body_original\n",
    "display(df1.head())\n",
    "df1 = pd.get_dummies(df1, columns = ['sentiment'], drop_first = False) # drop_first is false to get all three possibilities (column sentiments)\n",
    "\n",
    "df2 = df1\n",
    "\n",
    "df2 = df2[['sentiment_Bullish', 'sentiment_None' , 'sentiment_Bearish', 'body_original']] #reorders the columns\n",
    "\n",
    "print(df2.columns)\n",
    "\n",
    "print('Data Prep 1 completed. \\n')\n",
    "\n",
    "# DATA PREPARATION 2 - cleaning up the comments/tweets: Remove HTTP tags, Converts all to lower case, Removes punctuation;\n",
    "#Removes unicodes\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Remove HTTP tags\n",
    "%time df2['body_Processed'] = df2['body_original'].map(lambda x : ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))\n",
    "print('HTTP tags removed. \\n')\n",
    "\n",
    "#Lower Case\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x: x.lower())\n",
    "print('Converted to lower case. \\n')\n",
    "\n",
    "#Remove punctuations\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "print('Removed punctuatuations. \\n')\n",
    "\n",
    "#Remove unicodes\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "\n",
    "df2.head()\n",
    "print('Removed unicodes. \\n')\n",
    "\n",
    "print('Data Prep 2 completed. \\n')\n",
    "\n",
    "# DATA PREPARATION 3 - lemmatization/stopwords: Removes stopwords, Lemmatizes the words, Removes stopwords from lemmatizaation.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#adds new stopwords to list\n",
    "new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270',\n",
    "                  '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "                 '1600', '993', '392', '98', '00', '1601', 'amd', 'aapl', '03', '10', '100',\n",
    "                  '15', '18', '19', '20', '2021', '57', '0', '5', '11', 'qcom', 'hon', 'ibm',\n",
    "                 'intel', '05', '12', '13', '14', '17', '21', '22', '30', '50']\n",
    "\n",
    "new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', '00 call', '00 call entry',]\n",
    "\n",
    "\n",
    "new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270',\n",
    "                  '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "                 '1600', '993', '392', '98', '00', 'amd', 'aapl']\n",
    "\n",
    "\n",
    "for w in new_stop_words:\n",
    "    stop_words.append(w)\n",
    "\n",
    "print(stop_words)\n",
    "\n",
    "#removes the stopwords from the column body_Processed\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "df2.head()\n",
    "\n",
    "df = df2\n",
    "# Lemmatize the text\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "import nltk #not in original code\n",
    "nltk.download('wordnet') #not in original code\n",
    "\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "df2.head()\n",
    "\n",
    "#Removing Stop words again after Lemmatize\n",
    "%time df2['body_Processed'] = df2['body_Processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "display(df2.head())\n",
    "\n",
    "# Embedding on the processed text data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# BOW-TF Embedding\n",
    "\n",
    "no_features = 800\n",
    "tf_vectorizer = CountVectorizer(min_df=.015, max_df=.8, max_features=no_features, ngram_range=[1, 3])\n",
    "\n",
    "'''%time tpl_tf = tf_vectorizer.fit_transform(df2['body_Processed'])\n",
    "display(\"Bow-TF :\", tpl_tf.shape)\n",
    "df_tf = pd.DataFrame(tpl_tf.toarray(), columns=tf_vectorizer.get_feature_names())\n",
    "display(df_tf.head())'''\n",
    "'''\n",
    "#Preparing processed and BoW-TF embedded data for Classification\n",
    "df_tf_m = pd.concat([df2, df_tf], axis = 1)\n",
    "df_tf_m.drop(columns=['body_original', 'body_Processed'], inplace = True)\n",
    "print(df_tf_m.shape)\n",
    "display(df_tf_m.head())\n",
    "\n",
    "# BoW-TF:IDF Embedding\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=.02, max_df=.7, ngram_range=[1,3])\n",
    "\n",
    "%time tpl_tfidf = tfidf_vectorizer.fit_transform(df2['body_Processed'])\n",
    "display(\"Bow-TF:IDF :\", tpl_tfidf.shape)\n",
    "df_tfidf = pd.DataFrame(tpl_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names(), index=df2.index)\n",
    "display(df_tfidf.head())\n",
    "\n",
    "#Preparing processed and BoW-TF:IDF embedded data for Classification\n",
    "df_tfidf_m = pd.concat([df2, df_tfidf], axis = 1)\n",
    "df_tfidf_m.drop(columns=['body_original', 'body_Processed'], inplace = True)\n",
    "print(df_tfidf_m.shape)\n",
    "display(df_tfidf_m.head())'''\n",
    "\n",
    "'''from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier'''\n",
    "'''from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder'''\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, precision_score, recall_score,  accuracy_score, precision_recall_curve\n",
    "\n",
    "'''# USED TO SET UP THE TRAINING AND TESTING DATA SETS (USED IN MODEL CREATION)\n",
    "df = df_tf_m\n",
    "      \n",
    "Y = df['sentiment_Bullish']\n",
    "X = df.drop('sentiment_Bullish', axis = 1)\n",
    "    \n",
    "display('Y: ', Y)\n",
    "display('X: ', X)\n",
    "    \n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.85, random_state = 21)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(tpl_tf, Y, train_size = 0.85, random_state = 21)#NOTE: changing X to the vectorized body column corrects the dimensional mis-match.\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(tpl_tfidf, Y, train_size = 0.85, random_state = 21)#NOTE: changing X to the vectorized body column corrects the dimensional mis-match.\n",
    "\n",
    "print(\"Train Data Dimensions : \", X_train.shape)\n",
    "print(\"Test Data Dimensions : \", X_test.shape)'''\n",
    "      \n",
    "#HERE IS WHERE THE SAVED MODEL IS LOADED WITH PICKLE AND RUN\n",
    "print(\"\\n\", 'Naive Bayes Classifier')\n",
    "\n",
    "'''clf = MultinomialNB(alpha = 1.0)\n",
    "%time clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)\n",
    "clf_report(Y_test, Y_pred, probs)'''\n",
    "\n",
    "ModelFileToLoad = '2021-05-22 tech search stocktwitsModel.sav'\n",
    "VocabFileToLoad = '2021-05-22 tech search stocktwitsVocab.sav'\n",
    "\n",
    "    \n",
    "loaded_model = pickle.load(open(ModelFileToLoad, 'rb')) #The saved model is loaded\n",
    "loaded_vocab = pickle.load(open(VocabFileToLoad, 'rb')) #The saved vocab is loaded\n",
    "\n",
    "########\n",
    "# LOADS NEW DATA SET; PROCESSES THE NEW DATA; RUNS THE MODEL ON THE NEW DATA.\n",
    "########\n",
    "\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/Model Import Tests'\n",
    "df3 = getData(relevant_path + '/' + 'tech stockTwit 03112021.csv') #returns df; reads csv file into df\n",
    "\n",
    "print(df3.columns)\n",
    "\n",
    "#tpl_tf = tf_vectorizer.fit_transform(df2['\n",
    "\n",
    "#tf_vectorizer = CountVectorizer(vocabulary=tf_vectorizer.vocabulary_)\n",
    "tf_vectorizer = CountVectorizer(vocabulary=tpl_tf)\n",
    "\n",
    "\n",
    "df3 = DataPrep(df3)\n",
    "\n",
    "X_new = tf_vectorizer.transform(df3['body_Processed'])\n",
    "   \n",
    "Y_pred = loaded_model.predict(X_new) #Runs the model on the new data set\n",
    "\n",
    "display('New Y_pred: ', Y_pred)\n",
    "\n",
    "print(df3.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 4: Hyper-parameter tuning maodels that used TF-Bow embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid-Search hyperparameter tuing on AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_tf_m['sentiment_Bullish']\n",
    "X = df_tf_m.drop('sentiment_Bullish', axis = 1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.85, random_state = 21)\n",
    "print(\"Train Data Dimensions : \", X_train.shape)\n",
    "print(\"Test Data Dimensions : \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a grid of hyperparameters\n",
    "grid_params = {'n_estimators' : [100,200,300],\n",
    "               'learning_rate' : [1.0, 0.1, 0.05]}\n",
    "\n",
    "ABC = AdaBoostClassifier()\n",
    "#Building a 10 fold CV GridSearchCV object\n",
    "grid_object = GridSearchCV(estimator = ABC, param_grid = grid_params, scoring = 'roc_auc', cv = 10, n_jobs = -1)\n",
    "\n",
    "#Fitting the grid to the training data\n",
    "%time grid_object.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the best parameters and score\n",
    "print(\"Best Parameters : \", grid_object.best_params_)\n",
    "print(\"Best_ROC-AUC : \", round(grid_object.best_score_ * 100, 2))\n",
    "print(\"Best model : \", grid_object.best_estimator_)\n",
    "\n",
    "#Applying the tuned parameters back to the model\n",
    "Y_pred = grid_object.best_estimator_.predict(X_test)\n",
    "probs = grid_object.best_estimator_.predict_proba(X_test)\n",
    "clf_report(Y_test, Y_pred, probs)\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=25, shuffle=True)\n",
    "%time results = cross_val_score(grid_object.best_estimator_, X_test, Y_test, cv=kfold)\n",
    "results = results * 100\n",
    "results = np.round(results,2)\n",
    "print(\"Cross Validation Accuracy : \", round(results.mean(), 2))\n",
    "print(\"Cross Validation Accuracy in every fold : \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {'n_estimators' : [100,200,300,400,500],\n",
    "               'max_depth' : [10, 7, 5, 3],\n",
    "               'criterion' : ['entropy', 'gini']}\n",
    "\n",
    "RFC = RandomForestClassifier()\n",
    "grid_object = GridSearchCV(estimator = RFC, param_grid = grid_params, scoring = 'roc_auc', cv = 10, n_jobs = -1)\n",
    "\n",
    "%time grid_object.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best Parameters : \", grid_object.best_params_)\n",
    "print(\"Best_ROC-AUC : \", round(grid_object.best_score_ * 100, 2))\n",
    "print(\"Best model : \", grid_object.best_estimator_)\n",
    "\n",
    "Y_pred = grid_object.best_estimator_.predict(X_test)\n",
    "probs = grid_object.best_estimator_.predict_proba(X_test)\n",
    "clf_report(Y_test, Y_pred, probs)\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=25, shuffle=True)\n",
    "%time results = cross_val_score(grid_object.best_estimator_, X_test, Y_test, cv=kfold)\n",
    "results = results * 100\n",
    "results = np.round(results,2)\n",
    "print(\"Cross Validation Accuracy : \", round(results.mean(), 2))\n",
    "print(\"Cross Validation Accuracy in every fold : \", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### APPROACH 2: PyCaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('tech stockTwit 03112021-Copy1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop_duplicates() # removes duplicate records\n",
    "len(df1)\n",
    "\n",
    "column = 'symbol'\n",
    "df1.drop(df1[df1['symbol'] == column].index, inplace=True) #removes duplicate headers\n",
    "\n",
    "df1 = df1.reset_index(drop = True) # resets the index\n",
    "\n",
    "df1.drop(['symbol', 'created_at', 'followers'], inplace = True, axis = 1) #deletes columns\n",
    "len(df1)\n",
    "\n",
    "df1.rename(columns = {'body' : 'body_original'}, inplace = True) #renmaes body as body_original\n",
    "display(df1.head())\n",
    "df1 = pd.get_dummies(df1, columns = ['sentiment'], drop_first = False) # drop_first is false to get all three possibilities (column sentiments)\n",
    "\n",
    "df2 = df1\n",
    "\n",
    "display(df1.head())\n",
    "df2 = df2[['sentiment_Bullish', 'sentiment_None' , 'sentiment_Bearish', 'body_original']] #reorders the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --user https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.nlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time su_1 = setup(data = df1, target = 'body_original', custom_stopwords = stop_words, session_id = 21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Embedding on the processed text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time m1 = create_model(model='lda', multi_core=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lda_data = assign_model(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time m2 = create_model(model='nmf', multi_core=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time nmf_data = assign_model(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data.drop(['body_original', 'Dominant_Topic', 'Perc_Dominant_Topic'], axis=1, inplace = True)\n",
    "lda_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_data.drop(['body_original', 'Dominant_Topic', 'Perc_Dominant_Topic'], axis=1, inplace = True)\n",
    "nmf_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 3: Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pce_1 = setup(data = lda_data, target = 'sentiment_None', session_id = 5, train_size = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 4: Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1 : model creation\n",
    "#%time pce_1_m1 = create_model('rf') #original code does not define 'rf' needed for the next step\n",
    "%time rf = create_model('rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2 : model tuning\n",
    "%time tuned_pce_1_m1 = tune_model('rf')\n",
    "%time tuned_pce_1_m1 = tune_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step3 : getting insights from model perfromance\n",
    "%time evaluate_model(tuned_pce_1_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pce_2 = setup(data = nmf_data, target = 'sentiment_Bullish', session_id = 5, train_size = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2 : model tuning\n",
    "%time tuned_pce_2_m1 = tune_model(tuned_pce_1_m1, optimize='AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step3 : getting insights from model perfromance\n",
    "%time evaluate_model(tuned_pce_2_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to get 'top N' or 'bottom N' words\n",
    "#from PyCaret example\n",
    "\n",
    "def get_n_words(corpus, direction, n):\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    if direction == \"top\":\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    else:\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=False)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#10 most common and 10 most rare words\n",
    "common_words = get_n_words(df2['body_Processed'], \"top\", 15)\n",
    "rare_words = get_n_words(df2['body_Processed'], \"bottom\", 15)\n",
    "\n",
    "common_words = dict(common_words)\n",
    "names = list(common_words.keys())\n",
    "values = list(common_words.values())\n",
    "plt.subplots(figsize = (15,10))\n",
    "bars = plt.bar(range(len(common_words)),values,tick_label=names)\n",
    "plt.title('15 most common words:')\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x(), yval + .01, yval)\n",
    "plt.show()\n",
    "\n",
    "rare_words = dict(rare_words)\n",
    "names = list(rare_words.keys())\n",
    "values = list(rare_words.values())\n",
    "plt.subplots(figsize = (15,10))\n",
    "bars = plt.bar(range(len(rare_words)),values,tick_label=names)\n",
    "plt.title('15 most rare words:')\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x(), yval + .001, yval)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
