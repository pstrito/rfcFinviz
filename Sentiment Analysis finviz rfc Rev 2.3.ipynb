{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "Some things you must know before using this app.:\n",
    "\n",
    "    -This app helps survey the most recent 100 posts or titles that appear on the finviz.com website for a particular stock symbol. Much thanks to the Python Project for sharing their code in how to scrape the website and the use of nltk's Vader sentiment analyzer to provide the sentiment ratings for each of the titles. The visual summary in the form of a bar chart is also great. The link to the Youtube video is provided below.\n",
    "\n",
    "[Python Project] Sentiment Analysis and Visualization of Stock News - YouTube https://www.youtube.com/watch?v=o-zM8onpQZY\n",
    "\n",
    "    -The sentiment analyzer that is used is an industry standard but it is not perfect. You will find titles that appear to have their sentiments mis-rated.\n",
    "\n",
    "    -This is NOT a stock picker nor is it intended to provide any information on a company's or stock prices' future, present or past performance. Any conclusions made from the information contained here-in are the sole responsibility of the user.\n",
    "    \n",
    "    -The information contained here-in is not exhaustive and should not be treated as such.\n",
    "    \n",
    "    -The accuracy of the information contained here-in is not verified. It could be wrong. \n",
    "    \n",
    "    -This application assumes a rudimentry understanding of the jupyter notebook environment. If you have absolutely no technical background, find someone who does and who is willing to help you. It may save you hours of frustration. \n",
    "\n",
    "To use this app.:\n",
    "\n",
    "    Run the cell titled \"RUN THIS CELL\" by selecting this cell (side bar will be blue) and pressing the Run button on the toolbar above (in the jupyter notebook environment there are other ways to run the code.) \n",
    "\n",
    "    Enter the stock symbol (not the company's name) you want to research. (It is not case sensitive.)\n",
    "    \n",
    "    After you have entered the stock symbol you will be prompted to see if you want to remove what are known as stopwords. Stopwords are words that are generally accepted as conveying no sentiment. If you choose to have them removed it might make the the code run more efficiently.  \n",
    "    \n",
    "    You should see a bar chart with the sentiment values for the most recent 100 articles with date listed on the x-axis and the nltk's VADER sentiment value on the y-axis. If no bar chart appears run through the all of the prompts and re-start the app.\n",
    "    \n",
    "    Next you will see a pie chart that is divided into \"positive,\" \"neutral,\" and \"negative\" sentiments and the accompanied statistics.\n",
    "    \n",
    "    Following this you will see a word cloud of the titles with postive sentiment. \n",
    "    \n",
    "    To find the posts where these words reside, type in the word in the prompt. These are case sentitive so if your search comes back empty, re-enter the word with changed capitization. For example, start with the first letter of the word capitalized with the rest of the letters in lower case. If that does not yield anything, try all lower case. Make sure your spelling is exactly what is represented in the word cloud. Enter only one word at the prompt.\n",
    "    \n",
    "    If you don't want to search on any of the key words, just press \"enter\" when asked what word to search on.\n",
    "    \n",
    "    Next you will see the word cloud for the neutral sentiment titles. Follow the same procedure as above to search or move on.\n",
    "    \n",
    "    Next you will see the word cloud for the negative sentiment titles. You have the same opportunity to search titles that have negative sentiments.\n",
    "    \n",
    "    When the program is finished, the message \"All done ...\" will appear.\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Rev 1 incorporates simple statistics and sorting based on nltk sentiment ratings. si\n",
    "\n",
    "Rev 2 incorporates stopwords removal and then plots word clouds based on sentiment ratings.; adds pie chart; search titles\n",
    "using key word. si\n",
    "\n",
    "Rev 2.1 changes in the structure of the program. encorporates methods and main into one cell for easier execution.\n",
    "\n",
    "Rev 2.2 - makes the input fields more robust to aberrant inputs\n",
    "        - changes \"no\" to \"n\" in another key word search request\n",
    "        - incoporates the loading of the libraries into the main body for ease of execution\n",
    "        - adds instructions for running the application\n",
    "        \n",
    "Rev 2.3 - add capability to remove stopwords from the titles\n",
    "        - added histogram of sentiment values from the 100 articles (JI)\n",
    "        - added handling of empty article list for word cloud (if there are no articles, the word cloud is not made)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the symbol of the stock? (Please enter only one.)tsla\n",
      "Do you want to remove the stopwords from the titles? [press \"enter\" for no]y\n",
      "\n",
      "The stopwords will be removed. \n",
      "\n",
      "after tf_vectorizer\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialNB' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7f152b4099ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after tf_vectorizer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m \u001b[0mx_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfScrubbed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after x_new'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultinomialNB' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "#Title: RUN THIS CELL\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, precision_score, recall_score,  accuracy_score, precision_recall_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "#### METHODS ####\n",
    "\n",
    "def remove_stopwords(): #provides a comprehensive list of stopwords; returns 'stopWords'\n",
    "\n",
    "    #440\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    #450\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "\n",
    "    #print(len(stopWords))\n",
    "\n",
    "    #470 creates a list of new stopwords and then adds them to the set provided by nltk\n",
    "    # Note: it is case sensitive\n",
    "\n",
    "    newStopWords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "    newStopWords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "    newStopWords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "    newStopWords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "    newStopWords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "    newStopWords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "    newStopWords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "    newStopWords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "    newStopWords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
    "    newStopWords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "    newStopWords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "    newStopWords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "    newStopWords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "    newStopWords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "    newStopWords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "    newStopWords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "    newStopWords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "    newStopWords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "    newStopWords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "    newStopWords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "    newStopWords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "    newStopWords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "    newStopWords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "    newStopWords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "    newStopWords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "    newStopWords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "    newStopWords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "    newStopWords += ['nevertheless', 'next', 'nine', 'nobody', 'none'] #removed 'no'\n",
    "    newStopWords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "    newStopWords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "    newStopWords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "    newStopWords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "    newStopWords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "    newStopWords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "    newStopWords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "    newStopWords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "    newStopWords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "    newStopWords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "    newStopWords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "    newStopWords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "    newStopWords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "    newStopWords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "    newStopWords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "    newStopWords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "    newStopWords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "    newStopWords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "    newStopWords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "    newStopWords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "    newStopWords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
    "    newStopWords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
    "    newStopWords += ['yours', 'yourself', 'yourselves'] #provided by Codecademy??\n",
    "\n",
    "    # additional stopwords:\n",
    "    newStopWords += ['[Screenshot]', '[screenshot]', 'Screenshot', '[Screenshot]Great', '[SCREENSHOT]', 'screenshot', \n",
    "                 'The', 'the', 'SMART', 'yah', 'got', 'nutty', 'moving', 'weeks', 'Got', 'So', 'today', 'Been', 'or',\n",
    "                    \"n't\"]\n",
    "\n",
    "    newStopWords += ['I', 'it', 'It'] # pronouns\n",
    "\n",
    "    newStopWords += ['AMD', 'NVDA','NVDA', 'TSLA', 'GOOG', 'BA', 'FB', 'GOOGL', 'INTC', 'intel', 'Intel', 'CSCO', 'MU', \n",
    "                 'SMH', 'TSM','AAPL', 'TSLA', 'CSCO', 'POETF', 'PHOTONICS', 'DD', 'ARWR', 'T', 'INFI', 'AMC', 'ARK',\n",
    "                'GME', 'NIO', 'QS', 'ADBE', 'MSFT'] # Stock symbols or names\n",
    "\n",
    "    newStopWords += ['Readytogo123', 'Maddog68','Stocktwits'] # nouns\n",
    "\n",
    "    newStopWords += ['.', '?', '!', ';', ',', \"'\"] # punctuation\n",
    "\n",
    "    newStopWords += ['&', '#', '%', '$', '@'] # symbols\n",
    "\n",
    "    newStopWords += ['41.75', '530.05', '39', 'Two', 'two',] # numbers\n",
    "\n",
    "    #adds them to the stopWords list provided by nltk\n",
    "    for i in newStopWords:\n",
    "        stopWords.add(i) #stopWords is defined as a \"set\" in #450 when inputed as english words from nltk;\n",
    "        # sets cannot be ordered so it must be converted back to a list to be ordered or alphabetized. A set has no duplicate elements.\n",
    "\n",
    "    #print(len(stopWords))\n",
    "    #print(stopWords)\n",
    "\n",
    "    #converts the set to a list\n",
    "    stopWords_list = list(stopWords)\n",
    "\n",
    "    #sorts the stopword list\n",
    "    stopWords_list.sort(key = lambda k : k.lower())\n",
    "    #print(stopWords_list)\n",
    "    \n",
    "    \n",
    "    #480 This removes words from the list of stopwords and writes list to csv file\n",
    "    # https://stackoverflow.com/questions/29771168/how-to-remove-words-from-a-list-in-python#:~:text=one%20more%20easy%20way%20to%20remove%20words%20from,%3D%20words%20-%20stopwords%20final_list%20%3D%20list%20%28final_list%29\n",
    "    #new_words = list(filter(lambda w: w not in stop_words, initial_words))\n",
    "\n",
    "    WordsToBeRem = ['no'] #words to be removed from the stopword_list\n",
    "    stopWords = list(filter(lambda w: w not in WordsToBeRem, stopWords_list)) #stopWords_list has been sorted in #470\n",
    "\n",
    "    #converts the stopword list to a df and then outputs the df to a csv file\n",
    "    df_stopwords = pd.DataFrame(stopWords, columns = ['stopwords'])\n",
    "    df_stopwords.to_csv('stopwords.csv', index = False) #writes the csv file\n",
    "    \n",
    "    return stopWords\n",
    "\n",
    "def remove(df, stopWords): #returns a df where the stopwords are removed\n",
    "\n",
    "    dfScrubbed = df.copy() #This is a deep copy. df.copy(deep = True); deep = True is default\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(df):\n",
    "    \n",
    "        data = df.iloc[i,1] #column #1 holds the titles of the posts\n",
    "        words = word_tokenize(data) #the title is separated into individual words (tokenized)\n",
    "        wordsFiltered = []\n",
    "\n",
    "        for w in words:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "    \n",
    "        joinedWordsFiltered = ' '.join(wordsFiltered) #combines the individual words into one string\n",
    "    \n",
    "        dfScrubbed.iloc[i,1] = joinedWordsFiltered # replaces the recorded in dfAPIScrubbed with the stopWords removed\n",
    "        #from the 'body'\n",
    "    \n",
    "        i += 1\n",
    "    \n",
    "    #print(wordsFiltered)\n",
    "\n",
    "    #print(dfScrubbed.head())\n",
    "\n",
    "    return(dfScrubbed)\n",
    "\n",
    "def wc(df): #creates the word cloud\n",
    "    #from wordcloud import WordCloud, STOPWORDS \n",
    "    from wordcloud import WordCloud\n",
    "    import matplotlib.pyplot as plt \n",
    "    import pandas as pd \n",
    "\n",
    "    stopwords = set(stopWords) \n",
    "    words = ''\n",
    "    for review in df.title:\n",
    "        tokens = str(review).split()\n",
    "        tokens = [i.lower() for i in tokens]\n",
    "    \n",
    "        words += ' '.join(tokens) + ' '\n",
    "    \n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(words) \n",
    "  \n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "  \n",
    "    plt.show() \n",
    "\n",
    "def kw(df,keyword): # searches a string for key words; if found will print out the date and title\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "    \n",
    "        data = df.iloc[i,1] #column #1 holds the titles of the posts\n",
    "        a_bool = keyword in data\n",
    "\n",
    "        if a_bool == True:\n",
    "            print(df.iloc[i,0], df.iloc[i,1])\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "def search_repeat(df): # provides opportunity to do multiple searches on key words. returns only the appropriate yes or no response.\n",
    "    key_word = input('What is the key word you want to search? [press \"enter\" for none]')\n",
    "    if key_word:\n",
    "        kw(df, key_word)\n",
    "        answer = input('Do you want to do another search?')\n",
    "    else:\n",
    "        answer = 'no' #if there is not key word that is entered it sets answer to no. - assumes if there is no key word there is no desire to do another search.\n",
    "    while answer not in yes_answer and answer not in no_answer: # Restricts answer to be either in the yes or no list by continuous looping on it unit input matches either list\n",
    "        answer = error() # prompts for the correct yes or no response. The correct responses are in the yes_answer list and no_answer list.\n",
    "    return answer\n",
    "\n",
    "def error(): # provides user the opportunity to correct the user's input\n",
    "    correction = input('Your input needs to be either a \"y\" or a \"n\". Would you like to do another search?')\n",
    "    return correction\n",
    "\n",
    "def error1(): # provides user the opportunity to correct the user's input\n",
    "    correction = input('Your input needs to be either a \"y\" or a \"n\". Would you like to remove the stopwords from the titles?')\n",
    "    return correction\n",
    "\n",
    "def stopwords_yes_no(): # provides opportunity to removes stopwords from the titles. returns only the appropriate yes or no response.\n",
    "    yes_no = input('Do you want to remove the stopwords from the titles? [press \"enter\" for no]')\n",
    "    if yes_no in yes_answer:\n",
    "        answer = 'yes'\n",
    "    else:\n",
    "        answer = 'no' #if there is not key word that is entered it sets answer to no. - assumes if there is no key word there is no desire to do another search.\n",
    "    while answer not in yes_answer and answer not in no_answer: # Restricts answer to be either in the yes or no list by continuous looping on it unit input matches either list\n",
    "        answer = error1() # prompts for the correct yes or no response. The correct responses are in the yes_answer list and no_answer list.\n",
    "    return answer\n",
    "    \n",
    "#### MAIN ####\n",
    "\n",
    "yes_answer = ['yes','YES','Yes','y','Y']\n",
    "no_answer = ['no', 'NO', 'No', 'n', 'N']\n",
    "\n",
    "symbol = input('What is the symbol of the stock? (Please enter only one.)')\n",
    "\n",
    "#### SCRAPES FINVIZ\n",
    "\n",
    "finviz_url = 'https://www.finviz.com/quote.ashx?t='\n",
    "#tickers = ['NVDA', 'SLV', 'MU']\n",
    "tickers = [symbol]\n",
    "\n",
    "news_tables = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    url = finviz_url + ticker\n",
    "    \n",
    "    req = Request(url = url, headers = {'user-agent': 'my-app'})\n",
    "    response = urlopen(req)\n",
    "    \n",
    "    html = BeautifulSoup(response, 'html')\n",
    "    news_table = html.find(id = 'news-table')\n",
    "    news_tables[ticker] = news_table\n",
    "    \n",
    "parsed_data = []\n",
    "\n",
    "for ticker, news_table in news_tables.items():\n",
    "    for row in news_table.findAll('tr'):\n",
    "        \n",
    "        title = row.a.get_text()\n",
    "        date_data = row.td.text.split(' ')\n",
    "        \n",
    "        if len(date_data) == 1: # if there is both a date and time it parses them into two columns\n",
    "            time = date_data[0]\n",
    "        else: \n",
    "            date = date_data[0] \n",
    "            time = date_data[1]\n",
    "        parsed_data.append([ticker, date, time, title])\n",
    "        \n",
    "df = pd.DataFrame(parsed_data, columns = ['ticker', 'date', 'time', 'title'])\n",
    "\n",
    "#### REMOVES STOPWORDS\n",
    "\n",
    "sw_answer = stopwords_yes_no() #returns either a 'yes' or 'no' from the user's input\n",
    "if sw_answer == 'yes':\n",
    "    stopWords = remove_stopwords() #provides a comprehensive list of stopwords; returns 'stopWords'\n",
    "    dfScrubbed = remove(df, stopWords) #returns a df where the stopwords are removed\n",
    "    print('\\nThe stopwords will be removed. \\n')\n",
    "\n",
    "#### PERFORMS THE RANDOM FOREST CLASSIFIER SENTIMENT ANALYSIS.\n",
    "\n",
    "# Loads RFC model\n",
    "RFC_model = 'rfc_optimized_5-22 and 6-01 tech search stocktwits-Copy1 Model-Copy1.sav'\n",
    "RFC_loaded_model = pickle.load(open(RFC_model, 'rb')) #The saved RFC model is loaded\n",
    "\n",
    "\n",
    "RFC_vocab = 'rfc_optimized_5-22 and 6-01 tech search stocktwits-Copy1 Vocab-Copy1.sav'\n",
    "RFC_loaded_vocab = pickle.load(open(RFC_vocab, 'rb')) #The saved RFC vocab is loaded\n",
    "\n",
    "tf_vectorizer = RFC_loaded_vocab\n",
    "\n",
    "print('after tf_vectorizer')\n",
    "\n",
    "x_new = tf_vectorizer.transform(dfScrubbed['title'])\n",
    "\n",
    "print('after x_new')\n",
    "   \n",
    "###############################\n",
    "# RFC MODEL\n",
    "###############################\n",
    "RFC_Y_pred = RFC_loaded_model.predict(X_new) #Runs RFC model on the new data set\n",
    "df4 = pd.DataFrame(RFC_Y_pred, columns = ['Predicted Bullish Sentiment']) #creates a new df4 from the Y-Pred array\n",
    "RFC_dfPred = df2.join(df4) #joins the df2 and df4 dataframes togther\n",
    "\n",
    "display(RFC_dfPred.head())    \n",
    "    \n",
    "#### PERFORMS THE VADER SENTIMENT ANALYSIS.\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compound']\n",
    "\n",
    "if sw_answer == 'yes': #uses dfScrubbed to generate sentiment values if uses said yes\n",
    "    df['compound'] = dfScrubbed['title'].apply(f) # uses the scrubbed title data to generate the sentiment score and places the result back into the non-scrubbed df\n",
    "else:\n",
    "    df['compound'] = df['title'].apply(f) # does not uses the scrubbed titles to produce the sentiment values\n",
    "\n",
    "\n",
    "#print(df.head()) # commented out by si\n",
    "#print(len(df)) # commented out by si\n",
    "\n",
    "#### PLOTS SENTIMENT VALUES AS A FUNCTION OF DATES\n",
    "df ['date'] = pd.to_datetime(df.date).dt.date        \n",
    "        \n",
    "plt.figure(figsize = (10 ,8))\n",
    "\n",
    "mean_df = df.groupby(['ticker', 'date']).mean()\n",
    "mean_df = mean_df.unstack()\n",
    "mean_df = mean_df.xs('compound', axis = 'columns').transpose()\n",
    "\n",
    "mean_df.plot(kind = 'bar')\n",
    "\n",
    "print(tickers)\n",
    "plt.show()\n",
    "\n",
    "import time\n",
    "time.sleep(1.0)\n",
    "\n",
    "#### PROVIDES DATE RANGES AND SIMPLE STATISTICS ON THE SENTIMENT OF THE TITLES\n",
    "\n",
    "# provides date ranges for the last 100 articles; added by si\n",
    "print('Date Range of the 100 most recent articles: ') #added by si\n",
    "print('Most Recent Article Date: ', df.iloc[0,1]) #added by si\n",
    "#print('Oldest Article Date: ', df.iloc[99,1], '\\n') #added by si\n",
    "oldest = len(df) - 1\n",
    "print('Oldest Article Date: ', df.iloc[oldest,1], '\\n') #added by si\n",
    "\n",
    "\n",
    "# provides basic sentiment statistics; added by si\n",
    "i = 0 # set starting index number to 0\n",
    "pos_counter = 0 # sets starting positive counter to 0\n",
    "neu_counter = 0\n",
    "neg_counter = 0\n",
    "\n",
    "dfpos = pd.DataFrame(columns = ['date', 'title']) #initializes df where positive titles are stored\n",
    "dfneu = pd.DataFrame(columns = ['date', 'title'])\n",
    "dfneg = pd.DataFrame(columns = ['date', 'title'])\n",
    "\n",
    "# for the sentiment histogram\n",
    "sent_hist = []\n",
    "\n",
    "# Separate the sentiment values into pos, neu, and neg\n",
    "while i < len(df):\n",
    "    sent_hist.append(df.iloc[i,4]) # added for the sentiment histogram\n",
    "    if df.iloc[i,4] > 0.0:\n",
    "        pos_counter += 1\n",
    "        dfpos = dfpos.append(dict(zip(dfpos.columns,[df.iloc[i,1], df.iloc[i,3]])), ignore_index=True) #fill dfpos df\n",
    "\n",
    "    elif df.iloc[i,4] == 0.0:\n",
    "            neu_counter += 1\n",
    "            dfneu = dfneu.append(dict(zip(dfneu.columns,[df.iloc[i,1], df.iloc[i,3]])), ignore_index=True)\n",
    "\n",
    "    elif df.iloc[i,4] < 0.0:\n",
    "            neg_counter += 1\n",
    "            dfneg = dfneg.append(dict(zip(dfneg.columns,[df.iloc[i,1], df.iloc[i,3]])), ignore_index=True)\n",
    "            \n",
    "    i += 1\n",
    "    \n",
    "#### SENTIMENT HISTOGRAM\n",
    "sent_hist = np.asarray(sent_hist)\n",
    "plt.figure()\n",
    "#plt.hist(sent_hist, bins=20, range=[-1.0, 1.0])\n",
    "plt.hist(sent_hist, bins=[-1.0,-0.95,-0.85,-0.75, -0.65, -0.55, -0.45, -0.45, -0.35, -0.25, -0.15,\n",
    "                          -0.05, 0.05, 0.10, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, \n",
    "                          1.0], range = [-1.0, 1.0]) \n",
    "plt.title('Histogram of Sentiment Values')\n",
    "plt.xlabel('Sentiment Value')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "print('NOTE: The 0.0 bar contains both neutral and no comment sentiments. \\n')\n",
    "\n",
    "#### PIE CHART\n",
    "\n",
    "# Data to plot\n",
    "labels = 'Positive', 'Neutral', 'Negative'\n",
    "sizes = [pos_counter, neu_counter, neg_counter]\n",
    "colors = ['lightblue', 'orange', 'pink']\n",
    "explode = (0.1, 0, 0)  # explode 1st slice\n",
    "\n",
    "# Plot\n",
    "#plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "print(tickers)\n",
    "print('The percent of articles with Positive, Neutral and Negative sentiment.')\n",
    "plt.show()\n",
    "\n",
    "#### produces the word clouds; added by si\n",
    "print('\\n*****************')\n",
    "print('POSITIVE SENTIMENT: ')\n",
    "print('The number of positive sentiment numbers is: ', pos_counter)\n",
    "print('The percent of postive sentiment numbers is: ', pos_counter/len(df) * 100,'% \\n')\n",
    "\n",
    "time.sleep(1)\n",
    "if pos_counter != 0:\n",
    "    wc(remove(dfpos,remove_stopwords())) #creates the word cloud\n",
    "else:\n",
    "    print('There are no positive articles.')\n",
    "\n",
    "#### Title searches on key words for postive ratings\n",
    "\n",
    "time.sleep(1)\n",
    "repeat = 'yes' #initializes repeat to 'yes'; the user can/will change this in the search_repeat() method\n",
    "    \n",
    "while repeat in yes_answer:\n",
    "    repeat = search_repeat(dfpos)\n",
    "\n",
    "print('Moving on ...')\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "print('\\n*****************')\n",
    "print('NEUTRAL SENTIMENT:')\n",
    "print('The number of neutral sentiment numbers is: ', neu_counter)\n",
    "print('The percent of neutral sentiment numbers is: ', neu_counter/len(df) * 100,'% \\n')\n",
    "\n",
    "if neu_counter != 0:\n",
    "    wc(remove(dfneu,remove_stopwords())) #creates the word cloud\n",
    "else:\n",
    "    print('There are no neutral articles.')\n",
    "\n",
    "#### Title searches on key words for neutral ratings\n",
    "time.sleep(1)\n",
    "repeat = 'yes'\n",
    "while repeat in yes_answer:\n",
    "    repeat = search_repeat(dfneu)\n",
    "\n",
    "print('Moving on ...')\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "print('\\n*****************')\n",
    "print('NEGATIVE SENTIMENT: ')\n",
    "print('The number of negative sentiment numbers is: ', neg_counter)\n",
    "print('The percent of negativetive sentiment numbers is: ', neg_counter/len(df) * 100,'% \\n')\n",
    "\n",
    "if neg_counter != 0:\n",
    "    wc(remove(dfneg,remove_stopwords())) #creates the word cloud\n",
    "else:\n",
    "    print('There are no negative articles.')\n",
    "\n",
    "#### Title searches on key words for negative ratings\n",
    "time.sleep(1)\n",
    "repeat = 'yes'\n",
    "while repeat in yes_answer:\n",
    "    repeat = search_repeat(dfneg)\n",
    "\n",
    "print('All done ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
